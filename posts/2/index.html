<!doctype html><html lang=en style=font-size:120%><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content><meta name=description content="Deadlock is a tale as old as concurrency, and I&rsquo;ve personally seen a fair share of it working with conventional, relational databases. But most recently, I stumbled upon a deadlock in h2
The issue
With a really high max concurrent stream count with a huge payload in HTTP/2, combined with a high number of Futures in a single connection, hyper client quickly hung. The time it took to get stuck seems random at first, but after tweaking some settings here and there, I got it to hang after 5 request-response most of the time."><link rel=apple-touch-icon sizes=180x180 href=https://dswij.github.io//images/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://dswij.github.io//images/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://dswij.github.io//images/favicon-16x16.png><meta name=keywords content='hugo latex theme blog texify texify2 texify3 michael neuper'><link rel=stylesheet href=/katex/katex.min.css><script defer defer src=/katex/katex.min.js></script><script defer src=/katex/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://dswij.github.io/posts/2/"><meta property="og:site_name" content="dswij"><meta property="og:title" content="Deadlocks in the wild"><meta property="og:description" content="Deadlock is a tale as old as concurrency, and I’ve personally seen a fair share of it working with conventional, relational databases. But most recently, I stumbled upon a deadlock in h2
The issue With a really high max concurrent stream count with a huge payload in HTTP/2, combined with a high number of Futures in a single connection, hyper client quickly hung. The time it took to get stuck seems random at first, but after tweaking some settings here and there, I got it to hang after 5 request-response most of the time."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-08T00:00:00+00:00"><meta property="article:modified_time" content="2026-01-14T05:36:36+08:00"><link rel=canonical href=https://dswij.github.io/posts/2/><meta itemprop=name content="Deadlocks in the wild"><meta itemprop=description content="Deadlock is a tale as old as concurrency, and I’ve personally seen a fair share of it working with conventional, relational databases. But most recently, I stumbled upon a deadlock in h2
The issue With a really high max concurrent stream count with a huge payload in HTTP/2, combined with a high number of Futures in a single connection, hyper client quickly hung. The time it took to get stuck seems random at first, but after tweaking some settings here and there, I got it to hang after 5 request-response most of the time."><meta itemprop=datePublished content="2024-03-08T00:00:00+00:00"><meta itemprop=dateModified content="2026-01-14T05:36:36+08:00"><meta itemprop=wordCount content="676"><link rel=stylesheet href=/css/common.min.e562d763c6d0825495eb17de8b2c1d9800cf7c08db1c36accedf77a5fccfc4b9.css integrity="sha256-5WLXY8bQglSV6xfeiywdmADPfAjbHDaszt93pfzPxLk=" crossorigin=anonymous><link rel=stylesheet href=/css/content.min.6c9ed0934a51a1ab8000e08ffe5936c990057489fed352ce34606a43c03bf7d3.css integrity="sha256-bJ7Qk0pRoauAAOCP/lk2yZAFdIn+01LONGBqQ8A799M=" crossorigin=anonymous><title>Deadlocks in the wild - dswij</title><meta name=twitter:card content="summary"><meta name=twitter:title content="Deadlocks in the wild"><meta name=twitter:description content="Deadlock is a tale as old as concurrency, and I’ve personally seen a fair share of it working with conventional, relational databases. But most recently, I stumbled upon a deadlock in h2
The issue With a really high max concurrent stream count with a huge payload in HTTP/2, combined with a high number of Futures in a single connection, hyper client quickly hung. The time it took to get stuck seems random at first, but after tweaking some settings here and there, I got it to hang after 5 request-response most of the time."><link rel=stylesheet href=/css/single.min.be779f459ad7e3aaf8afd0f80c6a61ca6c50993f5f18512532100ac6d93f0fa9.css integrity="sha256-vnefRZrX46r4r9D4DGphymxQmT9fGFElMhAKxtk/D6k=" crossorigin=anonymous></head><body><div id=wrapper><header id=header><h1><a href=https://dswij.github.io/>dswij</a>
<button id=dark-mode-toggle class=dark-mode-toggle aria-label="Toggle theme">
<svg width="2rem" height="2rem" viewBox="0 0 496 496"><path fill="currentColor" d="M8 256C8 393 119 504 256 504S504 393 504 256 393 8 256 8 8 119 8 256zM256 440V72a184 184 0 010 368z" transform="translate(-8 -8)"/></svg></button></h1><nav><span class=nav-bar-item><span class=icon><svg height="1em" width="1em" id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 155.139 155.139"><g><g><path style="fill:#010002" d="M125.967 51.533V20.314h-16.862V38.06L77.569 12.814.0 74.869h21.361v67.455h112.416v-67.45h21.361L125.967 51.533zM125.925 134.472H97.546V82.37H57.58v52.103H29.202V71.146l48.356-38.689 48.355 38.689v63.326H125.925z"/></svg>
</span><a class=link href=/>About</a>
</span><span class=nav-bar-item><span class=icon><svg width="1em" height="1em" viewBox="0 0 24 24"><path d="M17 11H16a1 1 0 000 2h1a1 1 0 000-2zm0 4H16a1 1 0 000 2h1a1 1 0 000-2zM11 9h6a1 1 0 000-2H11a1 1 0 000 2zM21 3H7A1 1 0 006 4V7H3A1 1 0 002 8V18a3 3 0 003 3H18a4 4 0 004-4V4A1 1 0 0021 3zM6 18a1 1 0 01-2 0V9H6zm14-1a2 2 0 01-2 2H7.82A3 3 0 008 18V5H20zm-9-4h1a1 1 0 000-2H11a1 1 0 000 2zm0 4h1a1 1 0 000-2H11a1 1 0 000 2z"/></svg>
</span><a class=link href=/posts/>Blog</a></span></nav></header><hr class=head-rule></hr><main id=main class=post><div class=post-heading><h1 class=post-title>Deadlocks in the wild</h1><div class=publish-metadata><svg width=".75em" height=".75em" viewBox="0 0 24 24" fill="none"><path d="M3 9H21M7 3V5M17 3V5M6 13H8M6 17H8m3-4h2m-2 4h2m3-4h2m-2 4h2M6.2 21H17.8c1.1201.0 1.6802.0 2.108-.218C20.2843 20.5903 20.5903 20.2843 20.782 19.908 21 19.4802 21 18.9201 21 17.8V8.2c0-1.12011.0-1.68016-.218-2.10798C20.5903 5.71569 20.2843 5.40973 19.908 5.21799 19.4802 5 18.9201 5 17.8 5H6.2c-1.1201.0-1.68016.0-2.10798.21799C3.71569 5.40973 3.40973 5.71569 3.21799 6.09202 3 6.51984 3 7.07989 3 8.2v9.6c0 1.1201.0 1.6802.21799 2.108C3.40973 20.2843 3.71569 20.5903 4.09202 20.782 4.51984 21 5.07989 21 6.2 21z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg>
8 March 2024
&nbsp;
<span><svg width=".75em" height=".75em" viewBox="0 0 24 24" fill="none"><path d="M15.4998 5.49994l2.8284 2.82843M3 20.9997l.04745-.3322c.16791-1.1753.25187-1.763.44284-2.3117C3.65975 17.8689 3.89124 17.4059 4.17906 16.9783 4.50341 16.4963 4.92319 16.0765 5.76274 15.237L17.4107 3.58896c.781100000000002-.78105 2.0474-.78105 2.8285.0C21.0202 4.37001 21.0202 5.63634 20.2392 6.41739L8.37744 18.2791C7.61579 19.0408 7.23497 19.4216 6.8012 19.7244 6.41618 19.9932 6.00093 20.2159 5.56398 20.3879 5.07171 20.5817 4.54375 20.6882 3.48793 20.9012L3 20.9997z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg>
676 words
</span>&nbsp;
<span><svg width=".75em" height=".75em" viewBox="0 0 24 24" fill="none"><path d="M12 7v5l2.5 1.5M21 12c0 4.9706-4.0294 9-9 9-4.97056.0-9-4.0294-9-9 0-4.97056 4.02944-9 9-9 4.9706.0 9 4.02944 9 9z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg>
~4 mins</span></div></div><details><summary><b>Table of Contents</b></summary><div class="toc numbered-subtitles"><nav id=TableOfContents><ul><li><a href=#the-issue>The issue</a></li><li><a href=#debugging-it>Debugging it</a></li><li><a href=#the-fix>The fix</a></li><li><a href=#what-now>What now?</a></li></ul></nav></div></details><article class="content numbered-subtitles"><p>Deadlock is a tale as old as concurrency, and I&rsquo;ve personally seen a fair share of it working with conventional, relational databases. But most recently, I stumbled upon a deadlock in <a href=https://github.com/hyperium/h2><code>h2</code></a></p><h2 id=the-issue>The issue</h2><p>With a really high max concurrent stream count with a huge payload in HTTP/2, combined with a high number of <code>Futures</code> in a single connection, <code>hyper</code> client quickly hung. The time it took to get stuck seems random at first, but after tweaking some settings here and there, I got it to hang after 5 request-response most of the time.</p><p>Number 5 seems weird. A coincidence, perhaps?</p><h2 id=debugging-it>Debugging it</h2><p>I decided to enable logging with <code>tracing_subscriber</code> and found out that it always hangs after it got into a certain state. More precisely, it happens when a stream tried to send a payload, but can&rsquo;t get enough connection capacity. HTTP/2 introduced a flow control strategy where it is basically a mechanism to handle connection and stream capacities on top of the TCP layer. This allows multiple streams in a single connection to stream data concurrenctly in any order.</p><p>With that in mind, let&rsquo;s look at the log. The funny thing is that it always hangs soon after this line: <code>stream capacity is 0</code>.</p><pre tabindex=0><code class=language-log data-lang=log>2023-11-18T16:49:56.726594Z TRACE Connection{peer=Client}:poll:pop_frame:popped{stream.id=StreamId(517) stream.state=State { inner: HalfClosedLocal(AwaitingHeaders) }}: h2::proto::streams::prioritize: stream capacity is 0
2023-11-18T16:49:56.726599Z TRACE Connection{peer=Client}:poll:FramedWrite::flush: h2::codec::framed_write: flushing buffer
2023-11-18T16:49:56.726603Z TRACE Connection{peer=Client}:poll: tokio::task::waker: op=&#34;waker.clone&#34; task.id=5
2023-11-18T16:49:56.726607Z TRACE Connection{peer=Client}:poll: tokio::task::waker: op=&#34;waker.drop&#34; task.id=5

# HANGS
</code></pre><p>Interesting. Let&rsquo;s dive into the code for a bit:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span>    <span style=color:#66d9ef>if</span> sz <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>&amp;&amp;</span> stream_capacity <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span> {
</span></span><span style=display:flex><span>        tracing::<span style=color:#a6e22e>trace!</span>(<span style=color:#e6db74>&#34;stream capacity is 0&#34;</span>);
</span></span><span style=display:flex><span>        stream.pending_send.push_front(buffer, frame.into());
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>continue</span>;
</span></span><span style=display:flex><span>    }
</span></span></code></pre></div><p>It just run out of things that can be sent immediately because we don&rsquo;t have any capacity to send. Well, where did all the connection capacity go? Let&rsquo;s look at how we assign the pending capacity then:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span>    <span style=color:#75715e>// on receive WINDOW_UPDATE
</span></span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> stream.send_flow.available() <span style=color:#f92672>&lt;</span> stream.requested_send_capacity <span style=color:#66d9ef>as</span> <span style=color:#66d9ef>usize</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&amp;&amp;</span> stream.send_flow.has_unavailable()
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#75715e>// The stream requires additional capacity and the stream&#39;s
</span></span></span><span style=display:flex><span>        <span style=color:#75715e>// window has available capacity, but the connection window
</span></span></span><span style=display:flex><span>        <span style=color:#75715e>// does not.
</span></span></span><span style=display:flex><span>        <span style=color:#75715e>//
</span></span></span><span style=display:flex><span>        <span style=color:#75715e>// In this case, the stream needs to be queued up for when the
</span></span></span><span style=display:flex><span>        <span style=color:#75715e>// connection has more capacity.
</span></span></span><span style=display:flex><span>        self.pending_capacity.push(stream);
</span></span><span style=display:flex><span>    }
</span></span></code></pre></div><p>prying on <code>pending_capacity</code>, there&rsquo;s not only <code>max_concurrent_stream</code> amount of stream there. All the <code>Futures</code> are in pending_capacity. Aha! Now the deadlock sensor is tingling all over the place.</p><p>Ok, that&rsquo;s cool. But I hadn&rsquo;t really figure out why it got stuck after 5 request or why it got stuck at all! Yes, the capacity is poorly placed to non-sending streams, but that&rsquo;s all we have right now.</p><p>Upon further inspection, there are a couple of things that stood out:</p><ol><li>the server sent back WINDOW_UPDATE frames in small increments.</li><li>the payload max size is <code>5 * the max body buffer size</code>.</li><li>streams are put back to <code>pending_capacity</code> in a LIFO-manner.</li></ol><p>That&rsquo;s the problem. The inert Futures waiting in <code>pending_capacity</code> is just hogging all the connection capacity. We&rsquo;re left without any capacity for sending messages in send-ready streams.</p><h2 id=the-fix>The fix</h2><p>A really easy fix is to probably to put those ready to send in front of the queue. And it works perfectly fine.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span>    <span style=color:#75715e>// if the stream needs capacity we add:
</span></span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> stream.is_send_ready() {
</span></span><span style=display:flex><span>        <span style=color:#75715e>// Prioritize assigning capacity to a send-ready stream
</span></span></span><span style=display:flex><span>        self.pending_capacity.push_front(stream);
</span></span><span style=display:flex><span>    } <span style=color:#66d9ef>else</span> {
</span></span><span style=display:flex><span>        self.pending_capacity.push(stream);
</span></span><span style=display:flex><span>    }
</span></span></code></pre></div><p>But this runs into a problem when all the send-ready streams sent some of their payload, while we receive a WINDOW_UPDATE frames. Those streams are not in <code>pending_capacity</code> when this happens and some of the connection capacity will go to those inert streams. This won&rsquo;t get to a deadlock, but the connection capacity will be distributed poorly.</p><p>Let&rsquo;s fix that again:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span>    <span style=color:#75715e>// on receiving WINDOW_UPDATE
</span></span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>!</span>stream.is_pending_open {
</span></span><span style=display:flex><span>        self.try_assign_capacity(stream);
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// somewhere in the code when we can open another stream
</span></span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#66d9ef>let</span> Some(<span style=color:#66d9ef>mut</span> stream) <span style=color:#f92672>=</span> self.pop_pending_open(store, counts) {
</span></span><span style=display:flex><span>        self.pending_send.push_front(<span style=color:#f92672>&amp;</span><span style=color:#66d9ef>mut</span> stream);
</span></span><span style=display:flex><span>        self.try_assign_capacity(<span style=color:#f92672>&amp;</span><span style=color:#66d9ef>mut</span> stream);
</span></span><span style=display:flex><span>    }
</span></span></code></pre></div><p>Done! that&rsquo;s neat.</p><h2 id=what-now>What now?</h2><p>Well, the flow control in HTTP/2 turns out to be challenging. Since <code>h2</code>&rsquo;s implementation is eager on assigning capacity, combining it with another <code>Semaphore</code> leads to another <a href=https://github.com/hyperium/hyper/issues/3559>deadlock</a>!</p><p>But that&rsquo;s for another post.</p></article><button onclick=topFunction() id=back-to-top title="Go to top">Back to Top</button><div class=paginator><a class=link href=https://dswij.github.io/posts/1/ title="Please, *blame*">← prev</a>
<a></a></div></main><footer id=footer><div><span></span></div><div><span>Copyright © 2026</span></div></footer></div><script src=https://dswij.github.io/js/script.js defer></script></body></html>